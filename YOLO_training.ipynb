{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanushri123/YOLOV8_Automated_drone_subsystem/blob/main/DP_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXrXUbCidCNB",
        "outputId": "359e6d8e-d1b3-4b59-b372-43c5294831cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/detect.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source ../input.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHV1Uu9Iekda",
        "outputId": "740c0c70-92d6-45af-b8fc-30b78b062d56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 v7.0-150-gbe61a64 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.3/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR-wFxu6sBH4",
        "outputId": "eb6bae9d-eb6c-4f91-bf9b-179c167bd1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anY-fc-i4V6j",
        "outputId": "e99c84b5-87d7-47b4-f3ce-8cbf631ec981"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 780M/780M [00:04<00:00, 179MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYsgXC5TqEZI",
        "outputId": "7f0a506c-db22-4232-8bb2-7e4074f68f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "image 1/2 /content/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 44.8ms\n",
            "image 2/2 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 1 tie, 47.1ms\n",
            "Speed: 0.6ms pre-process, 45.9ms inference, 57.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.50\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCmSeu4R0Uqe",
        "outputId": "f00d9f80-6b4b-4fec-bafa-f584de0c0dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=2, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB disk): 100% 128/128 [00:00<00:00, 93776.58it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB disk): 100% 128/128 [00:00<00:00, 26509.53it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9         0G    0.04998    0.08584    0.03591         56        640: 100% 64/64 [03:12<00:00,  3.01s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.47s/it]\n",
            "                   all        128        929      0.751      0.611      0.705      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9         0G    0.04913    0.07827     0.0348         29        640: 100% 64/64 [03:01<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.46s/it]\n",
            "                   all        128        929      0.754      0.618      0.689      0.444\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9         0G     0.0476    0.07516     0.0351         49        640: 100% 64/64 [03:02<00:00,  2.85s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.46s/it]\n",
            "                   all        128        929      0.742      0.621      0.691      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9         0G     0.0465    0.07092    0.03292         29        640: 100% 64/64 [03:02<00:00,  2.86s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:47<00:00,  1.49s/it]\n",
            "                   all        128        929      0.699      0.645      0.703      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9         0G    0.04846     0.0721    0.03164         30        640: 100% 64/64 [03:01<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:48<00:00,  1.52s/it]\n",
            "                   all        128        929      0.704      0.608      0.679      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9         0G    0.04514    0.07052    0.02925         51        640: 100% 64/64 [03:02<00:00,  2.85s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:47<00:00,  1.50s/it]\n",
            "                   all        128        929      0.655      0.666      0.703      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9         0G    0.04643    0.06377    0.02975         19        640: 100% 64/64 [03:04<00:00,  2.89s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.44s/it]\n",
            "                   all        128        929      0.709      0.635      0.707      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9         0G    0.04524    0.06862    0.02497         14        640: 100% 64/64 [03:01<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:45<00:00,  1.42s/it]\n",
            "                   all        128        929      0.788      0.614      0.735      0.444\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9         0G    0.04564    0.06643    0.02638         61        640: 100% 64/64 [03:00<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:47<00:00,  1.49s/it]\n",
            "                   all        128        929        0.7      0.675      0.739      0.452\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9         0G    0.04714    0.06775    0.02787         31        640: 100% 64/64 [03:01<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.45s/it]\n",
            "                   all        128        929      0.747      0.658       0.74      0.463\n",
            "\n",
            "10 epochs completed in 0.641 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.8MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.8MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:46<00:00,  1.45s/it]\n",
            "                   all        128        929       0.75      0.618      0.706      0.472\n",
            "                person        128        254      0.855      0.697      0.799       0.52\n",
            "               bicycle        128          6      0.961      0.333      0.527       0.33\n",
            "                   car        128         46      0.751      0.413      0.548      0.254\n",
            "            motorcycle        128          5      0.477        0.8      0.797      0.657\n",
            "              airplane        128          6      0.995          1      0.995       0.73\n",
            "                   bus        128          7      0.547      0.714      0.796      0.679\n",
            "                 train        128          3          1      0.579       0.83      0.488\n",
            "                 truck        128         12      0.698      0.333      0.414      0.185\n",
            "                  boat        128          6      0.923      0.333      0.446      0.189\n",
            "         traffic light        128         14      0.626      0.242       0.36      0.203\n",
            "             stop sign        128          2      0.778          1      0.995      0.895\n",
            "                 bench        128          9      0.687      0.491      0.719      0.336\n",
            "                  bird        128         16      0.904          1      0.991       0.65\n",
            "                   cat        128          4      0.902          1      0.995      0.759\n",
            "                   dog        128          9      0.851      0.638      0.913      0.635\n",
            "                 horse        128          2      0.802          1      0.995      0.747\n",
            "              elephant        128         17      0.964      0.882      0.924      0.707\n",
            "                  bear        128          1      0.678          1      0.995      0.995\n",
            "                 zebra        128          4      0.882          1      0.995      0.948\n",
            "               giraffe        128          9      0.869       0.74      0.943      0.737\n",
            "              backpack        128          6      0.727        0.5      0.603      0.293\n",
            "              umbrella        128         18      0.723      0.722      0.832      0.468\n",
            "               handbag        128         19      0.661      0.158      0.257      0.133\n",
            "                   tie        128          7          1      0.546      0.724      0.412\n",
            "              suitcase        128          4      0.853          1      0.995      0.523\n",
            "               frisbee        128          5      0.695        0.8      0.759      0.659\n",
            "                  skis        128          1      0.589          1      0.995      0.398\n",
            "             snowboard        128          7      0.837      0.738       0.86      0.555\n",
            "           sports ball        128          6      0.599      0.667      0.667      0.368\n",
            "                  kite        128         10      0.855        0.7      0.661      0.282\n",
            "          baseball bat        128          4      0.602        0.5      0.497      0.165\n",
            "        baseball glove        128          7      0.771      0.429      0.469      0.287\n",
            "            skateboard        128          5      0.727      0.544       0.62       0.42\n",
            "         tennis racket        128          7      0.606      0.663      0.614      0.338\n",
            "                bottle        128         18      0.573      0.333      0.545      0.275\n",
            "            wine glass        128         16      0.696      0.938      0.893      0.497\n",
            "                   cup        128         36      0.835      0.583      0.784        0.5\n",
            "                  fork        128          6          1      0.313      0.409      0.292\n",
            "                 knife        128         16      0.785      0.683      0.738      0.406\n",
            "                 spoon        128         22      0.654      0.364      0.551      0.303\n",
            "                  bowl        128         28      0.829       0.52       0.67       0.49\n",
            "                banana        128          1      0.692          1      0.995      0.201\n",
            "              sandwich        128          2          1          0      0.249      0.224\n",
            "                orange        128          4          1      0.487      0.995      0.678\n",
            "              broccoli        128         11      0.373      0.379      0.446      0.326\n",
            "                carrot        128         24      0.757      0.542        0.7      0.493\n",
            "               hot dog        128          2      0.269          1      0.828      0.762\n",
            "                 pizza        128          5      0.792        0.6      0.828      0.613\n",
            "                 donut        128         14      0.663          1      0.928       0.81\n",
            "                  cake        128          4      0.891          1      0.995      0.785\n",
            "                 chair        128         35       0.53      0.613      0.564      0.297\n",
            "                 couch        128          6      0.866      0.667      0.802      0.562\n",
            "          potted plant        128         14      0.759      0.677      0.781      0.482\n",
            "                   bed        128          3          1          0      0.597      0.323\n",
            "          dining table        128         13      0.848      0.431      0.596      0.368\n",
            "                toilet        128          2          1      0.835      0.995      0.846\n",
            "                    tv        128          2       0.53          1      0.995      0.796\n",
            "                laptop        128          3          1          0      0.483       0.25\n",
            "                 mouse        128          2          1          0     0.0503     0.0252\n",
            "                remote        128          8          1      0.618      0.676      0.504\n",
            "            cell phone        128          8      0.311       0.25      0.197      0.101\n",
            "             microwave        128          3      0.777          1      0.995      0.699\n",
            "                  oven        128          5      0.231        0.4      0.439      0.277\n",
            "                  sink        128          6      0.258      0.167      0.289      0.224\n",
            "          refrigerator        128          5      0.616        0.8      0.804      0.501\n",
            "                  book        128         29       0.52      0.241       0.31      0.128\n",
            "                 clock        128          9       0.75      0.889      0.908      0.707\n",
            "                  vase        128          2       0.41          1      0.828      0.745\n",
            "              scissors        128          1          1          0     0.0553     0.0111\n",
            "            teddy bear        128         21      0.874      0.659      0.811      0.478\n",
            "            toothbrush        128          5      0.787      0.749      0.862      0.557\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train YOLOv5s on COCO128 for 10 epochs\n",
        "!python train.py --img 640 --batch 2 --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVsfmrc4KGV7",
        "outputId": "3c79fa67-dddd-4230-e0ec-07caba17ac83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=5, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-150-gbe61a64 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005078125), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Firedp/labels/train.cache... 300 images, 0 backgrounds, 0 corrupt: 100% 300/300 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/drive/MyDrive/Firedp/images/train/fire.357.png: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB disk): 100% 300/300 [00:00<00:00, 4031.47it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Firedp/labels/val.cache... 10 images, 0 backgrounds, 0 corrupt: 100% 10/10 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB disk): 100% 10/10 [00:00<00:00, 21.01it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.09 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp5/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/19      1.22G     0.1186     0.0539          0         30        640: 100% 60/60 [00:15<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.34it/s]\n",
            "                   all         10         67    0.00367      0.164    0.00243   0.000486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/19      1.22G     0.1052    0.06386          0         37        640: 100% 60/60 [00:13<00:00,  4.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.76it/s]\n",
            "                   all         10         67    0.00726      0.313    0.00694    0.00148\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/19      1.22G    0.09619    0.06212          0         43        640: 100% 60/60 [00:12<00:00,  4.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.15it/s]\n",
            "                   all         10         67     0.0242      0.149     0.0173    0.00432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/19      1.22G    0.09033    0.06176          0         57        640: 100% 60/60 [00:12<00:00,  4.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.12it/s]\n",
            "                   all         10         67       0.12      0.164     0.0611     0.0116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/19      1.22G    0.08488    0.05942          0         32        640: 100% 60/60 [00:13<00:00,  4.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.83it/s]\n",
            "                   all         10         67      0.173      0.194     0.0835     0.0256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/19      1.22G    0.07977    0.05964          0         47        640: 100% 60/60 [00:12<00:00,  4.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.66it/s]\n",
            "                   all         10         67      0.141      0.328      0.135     0.0393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/19      1.22G    0.07417    0.06163          0         62        640: 100% 60/60 [00:13<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.82it/s]\n",
            "                   all         10         67      0.189      0.328      0.132      0.036\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/19      1.22G    0.07244    0.06069          0         56        640: 100% 60/60 [00:12<00:00,  4.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.12it/s]\n",
            "                   all         10         67      0.193      0.249      0.149     0.0378\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/19      1.22G    0.06879    0.05638          0         47        640: 100% 60/60 [00:12<00:00,  4.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.46it/s]\n",
            "                   all         10         67      0.184      0.328      0.139     0.0435\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/19      1.22G    0.06647    0.05909          0         32        640: 100% 60/60 [00:13<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.57it/s]\n",
            "                   all         10         67       0.19      0.418      0.149     0.0487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/19      1.22G    0.06579    0.06023          0         62        640: 100% 60/60 [00:13<00:00,  4.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.38it/s]\n",
            "                   all         10         67      0.348      0.406      0.242     0.0661\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/19      1.22G    0.06427     0.0599          0         50        640: 100% 60/60 [00:12<00:00,  4.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.46it/s]\n",
            "                   all         10         67      0.427      0.388      0.295      0.087\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/19      1.22G    0.06272    0.06009          0         39        640: 100% 60/60 [00:13<00:00,  4.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.35it/s]\n",
            "                   all         10         67      0.453      0.388      0.327     0.0857\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/19      1.22G     0.0626    0.05874          0         34        640: 100% 60/60 [00:13<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.91it/s]\n",
            "                   all         10         67      0.371      0.388       0.31     0.0818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/19      1.22G    0.06077    0.05599          0         25        640: 100% 60/60 [00:12<00:00,  4.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.89it/s]\n",
            "                   all         10         67      0.387      0.403      0.307     0.0961\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/19      1.22G     0.0609     0.0561          0         25        640: 100% 60/60 [00:13<00:00,  4.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.69it/s]\n",
            "                   all         10         67      0.373      0.435      0.302     0.0856\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/19      1.22G    0.05903    0.05645          0         46        640: 100% 60/60 [00:12<00:00,  4.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.72it/s]\n",
            "                   all         10         67      0.533      0.392      0.361     0.0997\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/19      1.22G    0.05856    0.05652          0         29        640: 100% 60/60 [00:13<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  7.84it/s]\n",
            "                   all         10         67      0.449      0.448      0.385      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/19      1.22G    0.05868    0.06001          0         49        640: 100% 60/60 [00:12<00:00,  4.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.98it/s]\n",
            "                   all         10         67      0.448      0.463      0.401      0.116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/19      1.22G    0.05888    0.05741          0         44        640: 100% 60/60 [00:12<00:00,  4.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.01it/s]\n",
            "                   all         10         67      0.445      0.403      0.356      0.107\n",
            "\n",
            "20 epochs completed in 0.079 hours.\n",
            "Optimizer stripped from runs/train/exp5/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp5/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp5/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.97it/s]\n",
            "                   all         10         67      0.462      0.463      0.396      0.114\n",
            "Results saved to \u001b[1mruns/train/exp5\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on COCO128 for 20 epochs\n",
        "!python train.py --img 640 --batch 5 --epochs 20 --data coco128.yaml --weights yolov5s.pt --cache disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odDEy9acM1Pa",
        "outputId": "3ac5ba67-0764-4bff-f245-ffd6268d2435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: train.py\n",
            "       [-h]\n",
            "       [--weights WEIGHTS]\n",
            "       [--cfg CFG]\n",
            "       [--data DATA]\n",
            "       [--hyp HYP]\n",
            "       [--epochs EPOCHS]\n",
            "       [--batch-size BATCH_SIZE]\n",
            "       [--imgsz IMGSZ]\n",
            "       [--rect]\n",
            "       [--resume [RESUME]]\n",
            "       [--nosave]\n",
            "       [--noval]\n",
            "       [--noautoanchor]\n",
            "       [--noplots]\n",
            "       [--evolve [EVOLVE]]\n",
            "       [--bucket BUCKET]\n",
            "       [--cache [CACHE]]\n",
            "       [--image-weights]\n",
            "       [--device DEVICE]\n",
            "       [--multi-scale]\n",
            "       [--single-cls]\n",
            "       [--optimizer {SGD,Adam,AdamW}]\n",
            "       [--sync-bn]\n",
            "       [--workers WORKERS]\n",
            "       [--project PROJECT]\n",
            "       [--name NAME]\n",
            "       [--exist-ok]\n",
            "       [--quad]\n",
            "       [--cos-lr]\n",
            "       [--label-smoothing LABEL_SMOOTHING]\n",
            "       [--patience PATIENCE]\n",
            "       [--freeze FREEZE [FREEZE ...]]\n",
            "       [--save-period SAVE_PERIOD]\n",
            "       [--seed SEED]\n",
            "       [--local_rank LOCAL_RANK]\n",
            "       [--entity ENTITY]\n",
            "       [--upload_dataset [UPLOAD_DATASET]]\n",
            "       [--bbox_interval BBOX_INTERVAL]\n",
            "       [--artifact_alias ARTIFACT_ALIAS]\n",
            "train.py: error: argument --batch-size: invalid int value: '8--epochs'\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on COCO128 for 120 epochs\n",
        "!python train.py --img 640 --batch 8--epochs 120 --data coco128.yaml --weights yolov5s.pt --cache disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3d2FU74Pmin"
      },
      "outputs": [],
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "# !python train.py --img 640 --batch 2 --epochs 10 --data coco121.yaml --weights yolov5s.pt --cache disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DAAZVpDVbNh",
        "outputId": "50672743-63b4-443d-c163-807997a6e216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=120, batch_size=5, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005078125), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Firedp/labels/train.cache... 300 images, 0 backgrounds, 0 corrupt: 100% 300/300 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/drive/MyDrive/Firedp/images/train/fire.357.png: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB disk): 100% 300/300 [00:00<00:00, 3941.47it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Firedp/labels/val.cache... 10 images, 0 backgrounds, 0 corrupt: 100% 10/10 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB disk): 100% 10/10 [00:00<00:00, 2709.32it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.09 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 120 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      0/119      1.22G     0.1186     0.0539          0         30        640: 100% 60/60 [00:24<00:00,  2.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.30it/s]\n",
            "                   all         10         67    0.00367      0.164    0.00243   0.000486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      1/119      1.22G      0.105      0.064          0         37        640: 100% 60/60 [00:13<00:00,  4.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.20it/s]\n",
            "                   all         10         67     0.0105      0.254    0.00943     0.0021\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      2/119      1.22G    0.09629    0.06132          0         43        640: 100% 60/60 [00:14<00:00,  4.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.63it/s]\n",
            "                   all         10         67     0.0619      0.119     0.0233    0.00611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      3/119      1.22G    0.09072    0.06096          0         57        640: 100% 60/60 [00:14<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.87it/s]\n",
            "                   all         10         67      0.106      0.239     0.0675     0.0186\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      4/119      1.22G    0.08502    0.05843          0         32        640: 100% 60/60 [00:14<00:00,  4.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.98it/s]\n",
            "                   all         10         67      0.114      0.269     0.0968     0.0278\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      5/119      1.22G    0.08063     0.0595          0         47        640: 100% 60/60 [00:13<00:00,  4.30it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.28it/s]\n",
            "                   all         10         67      0.225      0.269      0.147     0.0435\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      6/119      1.22G    0.07484     0.0608          0         62        640: 100% 60/60 [00:14<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.38it/s]\n",
            "                   all         10         67      0.155      0.328      0.124     0.0327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      7/119      1.22G     0.0722    0.06086          0         56        640: 100% 60/60 [00:13<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.47it/s]\n",
            "                   all         10         67      0.203      0.343      0.153     0.0433\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      8/119      1.22G    0.06958    0.05664          0         47        640: 100% 60/60 [00:13<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.84it/s]\n",
            "                   all         10         67      0.262      0.418      0.213     0.0604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      9/119      1.22G    0.06681    0.05917          0         32        640: 100% 60/60 [00:14<00:00,  4.27it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.76it/s]\n",
            "                   all         10         67      0.265      0.299      0.178     0.0593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     10/119      1.22G    0.06695     0.0601          0         62        640: 100% 60/60 [00:13<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.22it/s]\n",
            "                   all         10         67       0.43      0.313       0.22     0.0581\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     11/119      1.22G    0.06489     0.0597          0         50        640: 100% 60/60 [00:12<00:00,  4.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.55it/s]\n",
            "                   all         10         67      0.373      0.388      0.275     0.0859\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     12/119      1.22G     0.0654    0.05913          0         39        640: 100% 60/60 [00:13<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.97it/s]\n",
            "                   all         10         67      0.318      0.403      0.282     0.0804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     13/119      1.22G    0.06533    0.05824          0         34        640: 100% 60/60 [00:14<00:00,  4.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  7.23it/s]\n",
            "                   all         10         67      0.367      0.313      0.262     0.0752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     14/119      1.22G    0.06187    0.05566          0         25        640: 100% 60/60 [00:13<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.02it/s]\n",
            "                   all         10         67      0.305      0.239      0.197     0.0505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     15/119      1.22G    0.06379    0.05541          0         25        640: 100% 60/60 [00:13<00:00,  4.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.80it/s]\n",
            "                   all         10         67      0.377      0.343       0.23     0.0676\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     16/119      1.22G    0.06039      0.056          0         46        640: 100% 60/60 [00:13<00:00,  4.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.87it/s]\n",
            "                   all         10         67       0.43      0.493      0.344     0.0957\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     17/119      1.22G    0.06031    0.05577          0         29        640: 100% 60/60 [00:13<00:00,  4.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.99it/s]\n",
            "                   all         10         67      0.414      0.411      0.315     0.0884\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     18/119      1.22G    0.05959    0.05906          0         49        640: 100% 60/60 [00:13<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.27it/s]\n",
            "                   all         10         67      0.306      0.403      0.259      0.093\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     19/119      1.22G    0.05909    0.05805          0         61        640:  75% 45/60 [00:10<00:03,  4.09it/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 172, in __iter__\n",
            "    yield next(self.iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 1285, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.9/queue.py\", line 180, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 316, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 642, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 531, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5/train.py\", line 286, in train\n",
            "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1193, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1299, in close\n",
            "    self.display(pos=0)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1492, in display\n",
            "    self.sp(self.__str__() if msg is None else msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 347, in print_status\n",
            "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 340, in fp_write\n",
            "    fp.write(str(s))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/utils.py\", line 127, in inner\n",
            "    return func(*args, **kwargs)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on COCO128 for 120 epochs\n",
        "!python train.py --img 640 --batch 5 --epochs 120 --data coco128.yaml --weights yolov5s.pt --cache disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_2OIOATa0q0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWsVdah7a3Ks"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data)\n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tfuBI7TbNVb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM45KSaODWSu",
        "outputId": "5c6df639-29d5-42a8-b11c-281182ae9e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "weights_path = \"/content/best.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torch.load(weights_path, map_location=device)[\"model\"].float().eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "WXDNp0WbD9eZ",
        "outputId": "fe836af1-cba8-4a07-da1c-49ef76e37acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5333dc44812>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/best.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
